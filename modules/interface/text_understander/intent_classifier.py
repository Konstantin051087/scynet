# modules/interface/text_understander/intent_classifier.py

import logging
import asyncio
import os
from typing import Dict, Any, Optional, List, Tuple
from pathlib import Path

logger = logging.getLogger(__name__)

class IntentClassifier:
    def __init__(self, config_path: Optional[str] = None):
        self.logger = logger
        self.config_path = config_path
        self.model = None
        self.tokenizer = None
        self.is_initialized = False
        
        # –ü—É—Ç–∏ –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        self.local_model_path = Path("data/models/bert-base-multilingual-uncased")
        self.use_local_model = self.local_model_path.exists()
        
        self.logger.info(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {self.local_model_path}")
        self.logger.info(f"üìÅ –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–æ—Å—Ç—É–ø–Ω–∞: {self.use_local_model}")
        
        if self.use_local_model:
            self._check_model_files()
    
    def _check_model_files(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏"""
        required_files = [
            "config.json",
            "vocab.txt",
            "tokenizer_config.json"
        ]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª—ã –≤–µ—Å–æ–≤ (–º–æ–∂–µ—Ç –±—ã—Ç—å –æ–¥–∏–Ω –∏–∑ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤)
        weight_files = ["pytorch_model.bin", "model.safetensors", "tf_model.h5"]
        has_weights = any((self.local_model_path / file).exists() for file in weight_files)
        
        missing_files = []
        for file in required_files:
            if not (self.local_model_path / file).exists():
                missing_files.append(file)
        
        if missing_files:
            self.logger.warning(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏: {missing_files}")
            self.use_local_model = False
        elif not has_weights:
            self.logger.warning("‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ñ–∞–π–ª—ã –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏")
            self.use_local_model = False
        else:
            self.logger.info("‚úÖ –í—Å–µ —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç")
    
    async def initialize(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
        try:
            if self.use_local_model:
                await self._initialize_from_local()
            else:
                await self._initialize_from_huggingface()
                
            self.is_initialized = True
            self.logger.info("‚úÖ IntentClassifier –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ IntentClassifier: {e}")
            self.is_initialized = False
    
    async def _initialize_from_local(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
        self.logger.info(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {self.local_model_path}")
        
        try:
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∑–¥–µ—Å—å, —á—Ç–æ–±—ã –æ—Ç–ª–æ–∂–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –¥–æ –º–æ–º–µ–Ω—Ç–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            from transformers import AutoModelForSequenceClassification, AutoTokenizer
            
            self.model, self.tokenizer = await asyncio.get_event_loop().run_in_executor(
                None,
                self._load_local_model_sync
            )
            self.logger.info("‚úÖ –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
            self.logger.info("üîÑ –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ Hugging Face –∫–∞–∫ fallback...")
            await self._initialize_from_huggingface()
    
    async def _initialize_from_huggingface(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ Hugging Face Hub"""
        self.logger.info("üåê –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ Hugging Face Hub...")
        
        try:
            from transformers import AutoModelForSequenceClassification, AutoTokenizer
            
            self.model, self.tokenizer = await asyncio.get_event_loop().run_in_executor(
                None,
                self._load_huggingface_model_sync
            )
            self.logger.info("‚úÖ –ú–æ–¥–µ–ª—å –∏–∑ Hugging Face —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –∏–∑ Hugging Face: {e}")
            raise
    
    def _load_local_model_sync(self):
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        from transformers import AutoModelForSequenceClassification, AutoTokenizer
        
        self.logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑: {self.local_model_path}")
        
        model = AutoModelForSequenceClassification.from_pretrained(
            str(self.local_model_path),
            local_files_only=True,
            num_labels=5  # –ë–∞–∑–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –Ω–∞—á–∞–ª–∞
        )
        tokenizer = AutoTokenizer.from_pretrained(
            str(self.local_model_path), 
            local_files_only=True
        )
        
        return model, tokenizer
    
    def _load_huggingface_model_sync(self):
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ Hugging Face"""
        from transformers import AutoModelForSequenceClassification, AutoTokenizer
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∫—ç—à–∞ –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        cache_dir = Path("data/models/cache")
        cache_dir.mkdir(parents=True, exist_ok=True)
        
        model = AutoModelForSequenceClassification.from_pretrained(
            "bert-base-multilingual-uncased",
            cache_dir=str(cache_dir),
            num_labels=5
        )
        tokenizer = AutoTokenizer.from_pretrained(
            "bert-base-multilingual-uncased",
            cache_dir=str(cache_dir)
        )
        
        return model, tokenizer
    
    async def classify(self, text: str) -> Dict[str, Any]:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞–º–µ—Ä–µ–Ω–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not self.is_initialized:
            raise RuntimeError("IntentClassifier –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        try:
            # –ü—Ä–æ—Å—Ç–µ–π—à–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            # –í –±—É–¥—É—â–µ–º –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—É—é –ª–æ–≥–∏–∫—É
            result = await asyncio.get_event_loop().run_in_executor(
                None,
                self._classify_sync,
                text
            )
            
            return result
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
            return {
                "intent": "error",
                "confidence": 0.0,
                "text": text,
                "error": str(e)
            }
    
    def _classify_sync(self, text: str) -> Dict[str, Any]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        # –ë–∞–∑–æ–≤–∞—è –ª–æ–≥–∏–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—ã–µ –ø—Ä–∞–≤–∏–ª–∞, –ø–æ–∑–∂–µ –∑–∞–º–µ–Ω–∏–º –Ω–∞ –º–æ–¥–µ–ª—å
        
        text_lower = text.lower().strip()
        
        # –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        if any(word in text_lower for word in ['–ø—Ä–∏–≤–µ—Ç', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π', 'hello', 'hi']):
            return {
                "intent": "greeting",
                "confidence": 0.9,
                "text": text,
                "entities": []
            }
        elif any(word in text_lower for word in ['–ø–æ–∫–∞', '–¥–æ —Å–≤–∏–¥–∞–Ω–∏—è', 'bye', 'goodbye']):
            return {
                "intent": "farewell", 
                "confidence": 0.9,
                "text": text,
                "entities": []
            }
        elif any(word in text_lower for word in ['–ø–æ–≥–æ–¥', 'weather']):
            return {
                "intent": "weather_query",
                "confidence": 0.8,
                "text": text,
                "entities": []
            }
        elif any(word in text_lower for word in ['–≤—Ä–µ–º', 'time']):
            return {
                "intent": "time_query",
                "confidence": 0.8,
                "text": text,
                "entities": []
            }
        else:
            return {
                "intent": "general_query",
                "confidence": 0.5,
                "text": text,
                "entities": []
            }
    
    async def get_model_info(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        if not self.is_initialized:
            return {"status": "not_initialized"}
        
        return {
            "status": "initialized",
            "model_type": "bert-base-multilingual-uncased",
            "local_model": self.use_local_model,
            "model_path": str(self.local_model_path) if self.use_local_model else "huggingface_hub"
        }
    
    async def shutdown(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        self.model = None
        self.tokenizer = None
        self.is_initialized = False
        self.logger.info("‚úÖ IntentClassifier –∑–∞–≤–µ—Ä—à–∏–ª —Ä–∞–±–æ—Ç—É")